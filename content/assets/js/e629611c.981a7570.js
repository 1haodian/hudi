"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[6119],{3905:function(e,t,a){a.d(t,{Zo:function(){return c},kt:function(){return m}});var r=a(67294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=r.createContext({}),d=function(e){var t=r.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},c=function(e){var t=d(e.components);return r.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},p=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=d(a),m=n,h=p["".concat(s,".").concat(m)]||p[m]||u[m]||o;return a?r.createElement(h,i(i({ref:t},c),{},{components:a})):r.createElement(h,i({ref:t},c))}));function m(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=a.length,i=new Array(o);i[0]=p;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:n,i[1]=l;for(var d=2;d<o;d++)i[d]=a[d];return r.createElement.apply(null,i)}return r.createElement.apply(null,a)}p.displayName="MDXCreateElement"},81807:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return d},toc:function(){return c},default:function(){return p}});var r=a(87462),n=a(63366),o=(a(67294),a(3905)),i=["components"],l={title:"Streaming Ingestion",keywords:["hudi","deltastreamer","hoodiedeltastreamer"]},s=void 0,d={unversionedId:"hoodie_deltastreamer",id:"hoodie_deltastreamer",isDocsHomePage:!1,title:"Streaming Ingestion",description:"DeltaStreamer",source:"@site/docs/hoodie_deltastreamer.md",sourceDirName:".",slug:"/hoodie_deltastreamer",permalink:"/docs/next/hoodie_deltastreamer",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/docs/docs/hoodie_deltastreamer.md",version:"current",frontMatter:{title:"Streaming Ingestion",keywords:["hudi","deltastreamer","hoodiedeltastreamer"]},sidebar:"docs",previous:{title:"Writing Data",permalink:"/docs/next/writing_data"},next:{title:"Querying Data",permalink:"/docs/next/querying_data"}},c=[{value:"DeltaStreamer",id:"deltastreamer",children:[]},{value:"MultiTableDeltaStreamer",id:"multitabledeltastreamer",children:[]},{value:"Concurrency Control",id:"concurrency-control",children:[]},{value:"Checkpointing",id:"checkpointing",children:[]},{value:"Schema Providers",id:"schema-providers",children:[{value:"Schema Registry Provider",id:"schema-registry-provider",children:[]},{value:"JDBC Schema Provider",id:"jdbc-schema-provider",children:[]},{value:"File Based Schema Provider",id:"file-based-schema-provider",children:[]},{value:"Schema Provider with Post Processor",id:"schema-provider-with-post-processor",children:[]}]},{value:"Sources",id:"sources",children:[{value:"Distributed File System (DFS)",id:"distributed-file-system-dfs",children:[]},{value:"Kafka",id:"kafka",children:[]},{value:"S3 Events",id:"s3-events",children:[]},{value:"JDBC Source",id:"jdbc-source",children:[]},{value:"SQL Source",id:"sql-source",children:[]}]},{value:"Hudi Kafka Connect Sink",id:"hudi-kafka-connect-sink",children:[]}],u={toc:c};function p(e){var t=e.components,a=(0,n.Z)(e,i);return(0,o.kt)("wrapper",(0,r.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"deltastreamer"},"DeltaStreamer"),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"HoodieDeltaStreamer")," utility (part of hudi-utilities-bundle) provides the way to ingest from different sources such as DFS or Kafka, with the following capabilities."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Exactly once ingestion of new events from Kafka, ",(0,o.kt)("a",{parentName:"li",href:"https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide#_incremental_imports"},"incremental imports")," from Sqoop or output of ",(0,o.kt)("inlineCode",{parentName:"li"},"HiveIncrementalPuller")," or files under a DFS folder"),(0,o.kt)("li",{parentName:"ul"},"Support json, avro or a custom record types for the incoming data"),(0,o.kt)("li",{parentName:"ul"},"Manage checkpoints, rollback & recovery"),(0,o.kt)("li",{parentName:"ul"},"Leverage Avro schemas from DFS or Confluent ",(0,o.kt)("a",{parentName:"li",href:"https://github.com/confluentinc/schema-registry"},"schema registry"),"."),(0,o.kt)("li",{parentName:"ul"},"Support for plugging in transformations")),(0,o.kt)("p",null,"Command line options describe capabilities in more detail"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},"[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` --help\nUsage: <main class> [options]\nOptions:\n    --checkpoint\n      Resume Delta Streamer from this checkpoint.\n    --commit-on-errors\n      Commit even when some records failed to be written\n      Default: false\n    --compact-scheduling-minshare\n      Minshare for compaction as defined in\n      https://spark.apache.org/docs/latest/job-scheduling\n      Default: 0\n    --compact-scheduling-weight\n      Scheduling weight for compaction as defined in\n      https://spark.apache.org/docs/latest/job-scheduling\n      Default: 1\n    --continuous\n      Delta Streamer runs in continuous mode running source-fetch -> Transform\n      -> Hudi Write in loop\n      Default: false\n    --delta-sync-scheduling-minshare\n      Minshare for delta sync as defined in\n      https://spark.apache.org/docs/latest/job-scheduling\n      Default: 0\n    --delta-sync-scheduling-weight\n      Scheduling weight for delta sync as defined in\n      https://spark.apache.org/docs/latest/job-scheduling\n      Default: 1\n    --disable-compaction\n      Compaction is enabled for MoR table by default. This flag disables it\n      Default: false\n    --enable-hive-sync\n      Enable syncing to hive\n      Default: false\n    --filter-dupes\n      Should duplicate records from source be dropped/filtered out before\n      insert/bulk-insert\n      Default: false\n    --help, -h\n\n    --hoodie-conf\n      Any configuration that can be set in the properties file (using the CLI\n      parameter \"--propsFilePath\") can also be passed command line using this\n      parameter\n      Default: []\n    --max-pending-compactions\n      Maximum number of outstanding inflight/requested compactions. Delta Sync\n      will not happen unlessoutstanding compactions is less than this number\n      Default: 5\n    --min-sync-interval-seconds\n      the min sync interval of each sync in continuous mode\n      Default: 0\n    --op\n      Takes one of these values : UPSERT (default), INSERT (use when input is\n      purely new data/inserts to gain speed)\n      Default: UPSERT\n      Possible Values: [UPSERT, INSERT, BULK_INSERT]\n    --payload-class\n      subclass of HoodieRecordPayload, that works off a GenericRecord.\n      Implement your own, if you want to do something other than overwriting\n      existing value\n      Default: org.apache.hudi.common.model.OverwriteWithLatestAvroPayload\n    --props\n      path to properties file on localfs or dfs, with configurations for\n      hoodie client, schema provider, key generator and data source. For\n      hoodie client props, sane defaults are used, but recommend use to\n      provide basic things like metrics endpoints, hive configs etc. For\n      sources, referto individual classes, for supported properties.\n      Default: file:///Users/vinoth/bin/hoodie/src/test/resources/delta-streamer-config/dfs-source.properties\n    --schemaprovider-class\n      subclass of org.apache.hudi.utilities.schema.SchemaProvider to attach\n      schemas to input & target table data, built in options:\n      org.apache.hudi.utilities.schema.FilebasedSchemaProvider.Source (See\n      org.apache.hudi.utilities.sources.Source) implementation can implement\n      their own SchemaProvider. For Sources that return Dataset<Row>, the\n      schema is obtained implicitly. However, this CLI option allows\n      overriding the schemaprovider returned by Source.\n    --source-class\n      Subclass of org.apache.hudi.utilities.sources to read data. Built-in\n      options: org.apache.hudi.utilities.sources.{JsonDFSSource (default), \n      AvroDFSSource, AvroKafkaSource, CsvDFSSource, HiveIncrPullSource, \n      JdbcSource, JsonKafkaSource, ORCDFSSource, ParquetDFSSource, \n      S3EventsHoodieIncrSource, S3EventsSource, SqlSource}\n      Default: org.apache.hudi.utilities.sources.JsonDFSSource\n    --source-limit\n      Maximum amount of data to read from source. Default: No limit For e.g:\n      DFS-Source => max bytes to read, Kafka-Source => max events to read\n      Default: 9223372036854775807\n    --source-ordering-field\n      Field within source record to decide how to break ties between records\n      with same key in input data. Default: 'ts' holding unix timestamp of\n      record\n      Default: ts\n    --spark-master\n      spark master to use.\n      Default: local[2]\n  * --table-type\n      Type of table. COPY_ON_WRITE (or) MERGE_ON_READ\n  * --target-base-path\n      base path for the target hoodie table. (Will be created if did not exist\n      first time around. If exists, expected to be a hoodie table)\n  * --target-table\n      name of the target table in Hive\n    --transformer-class\n      subclass of org.apache.hudi.utilities.transform.Transformer. Allows\n      transforming raw source Dataset to a target Dataset (conforming to\n      target schema) before writing. Default : Not set. E:g -\n      org.apache.hudi.utilities.transform.SqlQueryBasedTransformer (which\n      allows a SQL query templated to be passed as a transformation function)\n")),(0,o.kt)("p",null,"The tool takes a hierarchically composed property file and has pluggable interfaces for extracting data, key generation and providing schema. Sample configs for ingesting from kafka and dfs are\nprovided under ",(0,o.kt)("inlineCode",{parentName:"p"},"hudi-utilities/src/test/resources/delta-streamer-config"),"."),(0,o.kt)("p",null,"For e.g: once you have Confluent Kafka, Schema registry up & running, produce some test data using (",(0,o.kt)("a",{parentName:"p",href:"https://docs.confluent.io/current/ksql/docs/tutorials/generate-custom-test-data"},"impressions.avro")," provided by schema-registry repo)"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},"[confluent-5.0.0]$ bin/ksql-datagen schema=../impressions.avro format=avro topic=impressions key=impressionid\n")),(0,o.kt)("p",null,"and then ingest it as follows."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},"[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n  --props file://${PWD}/hudi-utilities/src/test/resources/delta-streamer-config/kafka-source.properties \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n  --source-ordering-field impresssiontime \\\n  --target-base-path file:\\/\\/\\/tmp/hudi-deltastreamer-op \\ \n  --target-table uber.impressions \\\n  --op BULK_INSERT\n")),(0,o.kt)("p",null,"In some cases, you may want to migrate your existing table into Hudi beforehand. Please refer to ",(0,o.kt)("a",{parentName:"p",href:"/docs/migration_guide"},"migration guide"),"."),(0,o.kt)("h2",{id:"multitabledeltastreamer"},"MultiTableDeltaStreamer"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"HoodieMultiTableDeltaStreamer"),", a wrapper on top of ",(0,o.kt)("inlineCode",{parentName:"p"},"HoodieDeltaStreamer"),", enables one to ingest multiple tables at a single go into hudi datasets. Currently it only supports sequential processing of tables to be ingested and COPY_ON_WRITE storage type. The command line options for ",(0,o.kt)("inlineCode",{parentName:"p"},"HoodieMultiTableDeltaStreamer")," are pretty much similar to ",(0,o.kt)("inlineCode",{parentName:"p"},"HoodieDeltaStreamer")," with the only exception that you are required to provide table wise configs in separate files in a dedicated config folder. The following command line options are introduced"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},"  * --config-folder\n    the path to the folder which contains all the table wise config files\n    --base-path-prefix\n    this is added to enable users to create all the hudi datasets for related tables under one path in FS. The datasets are then created under the path - <base_path_prefix>/<database>/<table_to_be_ingested>. However you can override the paths for every table by setting the property hoodie.deltastreamer.ingestion.targetBasePath\n")),(0,o.kt)("p",null,"The following properties are needed to be set properly to ingest data using ",(0,o.kt)("inlineCode",{parentName:"p"},"HoodieMultiTableDeltaStreamer"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},"hoodie.deltastreamer.ingestion.tablesToBeIngested\n  comma separated names of tables to be ingested in the format <database>.<table>, for example db1.table1,db1.table2\nhoodie.deltastreamer.ingestion.targetBasePath\n  if you wish to ingest a particular table in a separate path, you can mention that path here\nhoodie.deltastreamer.ingestion.<database>.<table>.configFile\n  path to the config file in dedicated config folder which contains table overridden properties for the particular table to be ingested.\n")),(0,o.kt)("p",null,"Sample config files for table wise overridden properties can be found under ",(0,o.kt)("inlineCode",{parentName:"p"},"hudi-utilities/src/test/resources/delta-streamer-config"),". The command to run ",(0,o.kt)("inlineCode",{parentName:"p"},"HoodieMultiTableDeltaStreamer")," is also similar to how you run ",(0,o.kt)("inlineCode",{parentName:"p"},"HoodieDeltaStreamer"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},"[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieMultiTableDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n  --props file://${PWD}/hudi-utilities/src/test/resources/delta-streamer-config/kafka-source.properties \\\n  --config-folder file://tmp/hudi-ingestion-config \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n  --source-ordering-field impresssiontime \\\n  --base-path-prefix file:\\/\\/\\/tmp/hudi-deltastreamer-op \\ \n  --target-table uber.impressions \\\n  --op BULK_INSERT\n")),(0,o.kt)("p",null,"For detailed information on how to configure and use ",(0,o.kt)("inlineCode",{parentName:"p"},"HoodieMultiTableDeltaStreamer"),", please refer ",(0,o.kt)("a",{parentName:"p",href:"/blog/2020/08/22/ingest-multiple-tables-using-hudi"},"blog section"),"."),(0,o.kt)("h2",{id:"concurrency-control"},"Concurrency Control"),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"HoodieDeltaStreamer")," utility (part of hudi-utilities-bundle) provides ways to ingest from different sources such as DFS or Kafka, with the following capabilities."),(0,o.kt)("p",null,"Using optimistic_concurrency_control via delta streamer requires adding the above configs to the properties file that can be passed to the\njob. For example below, adding the configs to kafka-source.properties file and passing them to deltastreamer will enable optimistic concurrency.\nA deltastreamer job can then be triggered as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},"[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n  --props file://${PWD}/hudi-utilities/src/test/resources/delta-streamer-config/kafka-source.properties \\\n  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n  --source-ordering-field impresssiontime \\\n  --target-base-path file:\\/\\/\\/tmp/hudi-deltastreamer-op \\ \n  --target-table uber.impressions \\\n  --op BULK_INSERT\n")),(0,o.kt)("p",null,"Read more in depth about concurrency control in the ",(0,o.kt)("a",{parentName:"p",href:"/docs/concurrency_control"},"concurrency control concepts")," section"),(0,o.kt)("h2",{id:"checkpointing"},"Checkpointing"),(0,o.kt)("p",null,"HoodieDeltaStreamer uses checkpoints to keep track of what data has been read already so it can resume without needing to reprocess all data.\nWhen using a Kafka source, the checkpoint is the ",(0,o.kt)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/KAFKA/Offset+Management"},"Kafka Offset"),"\nWhen using a DFS source, the checkpoint is the 'last modified' timestamp of the latest file read.\nCheckpoints are saved in the .hoodie commit file as ",(0,o.kt)("inlineCode",{parentName:"p"},"deltastreamer.checkpoint.key"),"."),(0,o.kt)("p",null,"If you need to change the checkpoints for reprocessing or replaying data you can use the following options:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"--checkpoint")," will set ",(0,o.kt)("inlineCode",{parentName:"li"},"deltastreamer.checkpoint.reset_key")," in the commit file to overwrite the current checkpoint."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"--source-limit")," will set a maximum amount of data to read from the source. For DFS sources, this is max # of bytes read.\nFor Kafka, this is the max # of events to read.")),(0,o.kt)("h2",{id:"schema-providers"},"Schema Providers"),(0,o.kt)("p",null,"By default, Spark will infer the schema of the source and use that inferred schema when writing to a table. If you need\nto explicitly define the schema you can use one of the following Schema Providers below."),(0,o.kt)("h3",{id:"schema-registry-provider"},"Schema Registry Provider"),(0,o.kt)("p",null,"You can obtain the latest schema from an online registry. You pass a URL to the registry and if needed, you can also\npass userinfo and credentials in the url like: ",(0,o.kt)("inlineCode",{parentName:"p"},"https://foo:bar@schemaregistry.org")," The credentials are then extracted\nand are set on the request as an Authorization Header."),(0,o.kt)("p",null,"When fetching schemas from a registry, you can specify both the source schema and the target schema separately."),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Config"),(0,o.kt)("th",{parentName:"tr",align:null},"Description"),(0,o.kt)("th",{parentName:"tr",align:null},"Example"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.schemaprovider.registry.url"),(0,o.kt)("td",{parentName:"tr",align:null},"The schema of the source you are reading from"),(0,o.kt)("td",{parentName:"tr",align:null},"https://foo:",(0,o.kt)("a",{parentName:"td",href:"mailto:bar@schemaregistry.org"},"bar@schemaregistry.org"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.schemaprovider.registry.targetUrl"),(0,o.kt)("td",{parentName:"tr",align:null},"The schema of the target you are writing to"),(0,o.kt)("td",{parentName:"tr",align:null},"https://foo:",(0,o.kt)("a",{parentName:"td",href:"mailto:bar@schemaregistry.org"},"bar@schemaregistry.org"))))),(0,o.kt)("p",null,"The above configs are passed to DeltaStreamer spark-submit command like:\n",(0,o.kt)("inlineCode",{parentName:"p"},"--hoodie-conf hoodie.deltastreamer.schemaprovider.registry.url=https://foo:bar@schemaregistry.org")),(0,o.kt)("h3",{id:"jdbc-schema-provider"},"JDBC Schema Provider"),(0,o.kt)("p",null,"You can obtain the latest schema through a JDBC connection."),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Config"),(0,o.kt)("th",{parentName:"tr",align:null},"Description"),(0,o.kt)("th",{parentName:"tr",align:null},"Example"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.schemaprovider.source.schema.jdbc.connection.url"),(0,o.kt)("td",{parentName:"tr",align:null},"The JDBC URL to connect to. You can specify source specific connection properties in the URL"),(0,o.kt)("td",{parentName:"tr",align:null},"jdbc:postgresql://localhost/test?user=fred&password=secret")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.schemaprovider.source.schema.jdbc.driver.type"),(0,o.kt)("td",{parentName:"tr",align:null},"The class name of the JDBC driver to use to connect to this URL"),(0,o.kt)("td",{parentName:"tr",align:null},"org.h2.Driver")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.schemaprovider.source.schema.jdbc.username"),(0,o.kt)("td",{parentName:"tr",align:null},"username for the connection"),(0,o.kt)("td",{parentName:"tr",align:null},"fred")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.schemaprovider.source.schema.jdbc.password"),(0,o.kt)("td",{parentName:"tr",align:null},"password for the connection"),(0,o.kt)("td",{parentName:"tr",align:null},"secret")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.schemaprovider.source.schema.jdbc.dbtable"),(0,o.kt)("td",{parentName:"tr",align:null},"The table with the schema to reference"),(0,o.kt)("td",{parentName:"tr",align:null},"test_database.test1_table or test1_table")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.schemaprovider.source.schema.jdbc.timeout"),(0,o.kt)("td",{parentName:"tr",align:null},"The number of seconds the driver will wait for a Statement object to execute to the given number of seconds. Zero means there is no limit. In the write path, this option depends on how JDBC drivers implement the API setQueryTimeout, e.g., the h2 JDBC driver checks the timeout of each query instead of an entire JDBC batch. It defaults to 0."),(0,o.kt)("td",{parentName:"tr",align:null},"0")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.schemaprovider.source.schema.jdbc.nullable"),(0,o.kt)("td",{parentName:"tr",align:null},"If true, all columns are nullable"),(0,o.kt)("td",{parentName:"tr",align:null},"true")))),(0,o.kt)("p",null,"The above configs are passed to DeltaStreamer spark-submit command like:\n",(0,o.kt)("inlineCode",{parentName:"p"},"--hoodie-conf hoodie.deltastreamer.jdbcbasedschemaprovider.connection.url=jdbc:postgresql://localhost/test?user=fred&password=secret")),(0,o.kt)("h3",{id:"file-based-schema-provider"},"File Based Schema Provider"),(0,o.kt)("p",null,"You can use a .avsc file to define your schema. You can then point to this file on DFS as a schema provider."),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Config"),(0,o.kt)("th",{parentName:"tr",align:null},"Description"),(0,o.kt)("th",{parentName:"tr",align:null},"Example"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.schemaprovider.source.schema.file"),(0,o.kt)("td",{parentName:"tr",align:null},"The schema of the source you are reading from"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("a",{parentName:"td",href:"https://github.com/apache/hudi/blob/a8fb69656f522648233f0310ca3756188d954281/docker/demo/config/test-suite/source.avsc"},"example schema file"))),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.schemaprovider.target.schema.file"),(0,o.kt)("td",{parentName:"tr",align:null},"The schema of the target you are writing to"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("a",{parentName:"td",href:"https://github.com/apache/hudi/blob/a8fb69656f522648233f0310ca3756188d954281/docker/demo/config/test-suite/target.avsc"},"example schema file"))))),(0,o.kt)("h3",{id:"schema-provider-with-post-processor"},"Schema Provider with Post Processor"),(0,o.kt)("p",null,"The SchemaProviderWithPostProcessor, will extract the schema from one of the previously mentioned Schema Providers and\nthen will apply a post processor to change the schema before it is used. You can write your own post processor by extending\nthis class: ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/blob/master/hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaPostProcessor.java"},"https://github.com/apache/hudi/blob/master/hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaPostProcessor.java")),(0,o.kt)("h2",{id:"sources"},"Sources"),(0,o.kt)("p",null,"Hoodie DeltaStreamer can read data from a wide variety of sources. The following are a list of supported sources:"),(0,o.kt)("h3",{id:"distributed-file-system-dfs"},"Distributed File System (DFS)"),(0,o.kt)("p",null,"See the storage configurations page to see some examples of DFS applications Hudi can read from. The following are the\nsupported file formats Hudi can read/write with on DFS Sources. (Note: you can still use Spark/Flink readers to read from\nother formats and then write data as Hudi format.)"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"CSV"),(0,o.kt)("li",{parentName:"ul"},"AVRO"),(0,o.kt)("li",{parentName:"ul"},"JSON"),(0,o.kt)("li",{parentName:"ul"},"PARQUET"),(0,o.kt)("li",{parentName:"ul"},"ORC"),(0,o.kt)("li",{parentName:"ul"},"HUDI")),(0,o.kt)("h3",{id:"kafka"},"Kafka"),(0,o.kt)("p",null,"Hudi can read directly from Kafka clusters. See more details on HoodieDeltaStreamer to learn how to setup streaming\ningestion with exactly once semantics, checkpointing, and plugin transformations. The following formats are supported\nwhen reading data from Kafka:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"AVRO"),(0,o.kt)("li",{parentName:"ul"},"JSON")),(0,o.kt)("h3",{id:"s3-events"},"S3 Events"),(0,o.kt)("p",null,"AWS S3 storage provides an event notification service which will post notifications when certain events happen in your S3 bucket:\n",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html"},"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html"),"\nAWS will put these events in a Simple Queue Service (SQS). Apache Hudi provides an S3EventsSource that can read from SQS\nto trigger/processing of new or changed data as soon as it is available on S3."),(0,o.kt)("h4",{id:"setup"},"Setup"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Enable S3 Event Notifications ",(0,o.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html"},"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html")),(0,o.kt)("li",{parentName:"ol"},"Download the aws-java-sdk-sqs jar. "),(0,o.kt)("li",{parentName:"ol"},"Find the queue URL and Region to set these configurations:",(0,o.kt)("ol",{parentName:"li"},(0,o.kt)("li",{parentName:"ol"},"hoodie.deltastreamer.s3.source.queue.url=",(0,o.kt)("a",{parentName:"li",href:"https://sqs.us-west-2.amazonaws.com/queue/url"},"https://sqs.us-west-2.amazonaws.com/queue/url")),(0,o.kt)("li",{parentName:"ol"},"hoodie.deltastreamer.s3.source.queue.region=us-west-2 "))),(0,o.kt)("li",{parentName:"ol"},"start the S3EventsSource and S3EventsHoodieIncrSource using the HoodieDeltaStreamer utility as shown in sample commands below:")),(0,o.kt)("p",null,"Insert code sample from this blog: ",(0,o.kt)("a",{parentName:"p",href:"https://hudi.apache.org/blog/2021/08/23/s3-events-source/#configuration-and-setup"},"https://hudi.apache.org/blog/2021/08/23/s3-events-source/#configuration-and-setup")),(0,o.kt)("h3",{id:"jdbc-source"},"JDBC Source"),(0,o.kt)("p",null,"Hudi can read from a JDBC source with a full fetch of a table, or Hudi can even read incrementally with checkpointing from a JDBC source."),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Config"),(0,o.kt)("th",{parentName:"tr",align:null},"Description"),(0,o.kt)("th",{parentName:"tr",align:null},"Example"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.url"),(0,o.kt)("td",{parentName:"tr",align:null},"URL of the JDBC connection"),(0,o.kt)("td",{parentName:"tr",align:null},"jdbc:postgresql://localhost/test")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.user"),(0,o.kt)("td",{parentName:"tr",align:null},"User to use for authentication of the JDBC connection"),(0,o.kt)("td",{parentName:"tr",align:null},"fred")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.password"),(0,o.kt)("td",{parentName:"tr",align:null},"Password to use for authentication of the JDBC connection"),(0,o.kt)("td",{parentName:"tr",align:null},"secret")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.password.file"),(0,o.kt)("td",{parentName:"tr",align:null},"If you prefer to use a password file for the connection"),(0,o.kt)("td",{parentName:"tr",align:null})),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.driver.class"),(0,o.kt)("td",{parentName:"tr",align:null},"Driver class to use for the JDBC connection"),(0,o.kt)("td",{parentName:"tr",align:null})),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.table.name"),(0,o.kt)("td",{parentName:"tr",align:null}),(0,o.kt)("td",{parentName:"tr",align:null},"my_table")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.table.incr.column.name"),(0,o.kt)("td",{parentName:"tr",align:null},"If run in incremental mode, this field will be used to pull new data incrementally"),(0,o.kt)("td",{parentName:"tr",align:null})),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.incr.pull"),(0,o.kt)("td",{parentName:"tr",align:null},"Will the JDBC connection perform an incremental pull?"),(0,o.kt)("td",{parentName:"tr",align:null})),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.extra.options."),(0,o.kt)("td",{parentName:"tr",align:null},"How you pass extra configurations that would normally by specified as spark.read.option()"),(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.extra.options.fetchSize=100 hoodie.deltastreamer.jdbc.extra.options.upperBound=1 hoodie.deltastreamer.jdbc.extra.options.lowerBound=100")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.storage.level"),(0,o.kt)("td",{parentName:"tr",align:null},"Used to control the persistence level"),(0,o.kt)("td",{parentName:"tr",align:null},"Default = MEMORY_AND_DISK_SER")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"hoodie.deltastreamer.jdbc.incr.fallback.to.full.fetch"),(0,o.kt)("td",{parentName:"tr",align:null},"Boolean which if set true makes an incremental fetch fallback to a full fetch if there is any error in the incremental read"),(0,o.kt)("td",{parentName:"tr",align:null},"FALSE")))),(0,o.kt)("h3",{id:"sql-source"},"SQL Source"),(0,o.kt)("p",null,"SQL Source that reads from any table, used mainly for backfill jobs which will process specific partition dates.\nThis won't update the deltastreamer.checkpoint.key to the processed commit, instead it will fetch the latest successful\ncheckpoint key and set that value as this backfill commits checkpoint so that it won't interrupt the regular incremental\nprocessing. To fetch and use the latest incremental checkpoint, you need to also set this hoodie_conf for deltastremer\njobs: ",(0,o.kt)("inlineCode",{parentName:"p"},"hoodie.write.meta.key.prefixes = 'deltastreamer.checkpoint.key'")),(0,o.kt)("p",null,"Spark SQL should be configured using this hoodie config:\nhoodie.deltastreamer.source.sql.sql.query = 'select * from source_table'"),(0,o.kt)("h2",{id:"hudi-kafka-connect-sink"},"Hudi Kafka Connect Sink"),(0,o.kt)("p",null,"If you want to perform streaming ingestion into Hudi format similar to HoodieDeltaStreamer, but you don't want to depend on Spark,\ntry out the new experimental release of Hudi Kafka Connect Sink. Read the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/apache/hudi/tree/master/hudi-kafka-connect"},"ReadMe"),"\nfor full documentation."))}p.isMDXComponent=!0}}]);