"use strict";(self.webpackChunkhudi=self.webpackChunkhudi||[]).push([[719],{3905:function(e,t,n){n.d(t,{Zo:function(){return l},kt:function(){return d}});var o=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,o,i=function(e,t){if(null==e)return{};var n,o,i={},a=Object.keys(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=o.createContext({}),p=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},l=function(e){var t=p(e.components);return o.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},m=o.forwardRef((function(e,t){var n=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),m=p(n),d=i,h=m["".concat(s,".").concat(d)]||m[d]||u[d]||a;return n?o.createElement(h,r(r({ref:t},l),{},{components:n})):o.createElement(h,r({ref:t},l))}));function d(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=n.length,r=new Array(a);r[0]=m;var c={};for(var s in t)hasOwnProperty.call(t,s)&&(c[s]=t[s]);c.originalType=e,c.mdxType="string"==typeof e?e:i,r[1]=c;for(var p=2;p<a;p++)r[p]=n[p];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}m.displayName="MDXCreateElement"},13974:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return s},metadata:function(){return p},toc:function(){return l},default:function(){return m}});var o=n(87462),i=n(63366),a=(n(67294),n(3905)),r=["components"],c={title:"Compaction",summary:"In this page, we describe async compaction in Hudi.",toc:!0,last_modified_at:null},s=void 0,p={unversionedId:"compaction",id:"compaction",isDocsHomePage:!1,title:"Compaction",description:"Compaction is executed asynchronously with Hudi by default.",source:"@site/docs/compaction.md",sourceDirName:".",slug:"/compaction",permalink:"/cn/docs/next/compaction",editUrl:"https://github.com/apache/hudi/edit/asf-site/website/docs/docs/compaction.md",version:"current",frontMatter:{title:"Compaction",summary:"In this page, we describe async compaction in Hudi.",toc:!0,last_modified_at:null},sidebar:"docs",previous:{title:"Migration Guide",permalink:"/cn/docs/next/migration_guide"},next:{title:"Clustering",permalink:"/cn/docs/next/clustering"}},l=[{value:"Async Compaction",id:"async-compaction",children:[]},{value:"Deployment Models",id:"deployment-models",children:[{value:"Spark Structured Streaming",id:"spark-structured-streaming",children:[]},{value:"DeltaStreamer Continuous Mode",id:"deltastreamer-continuous-mode",children:[]},{value:"Hudi Compactor Utility",id:"hudi-compactor-utility",children:[]},{value:"Hudi CLI",id:"hudi-cli",children:[]}]},{value:"Synchronous Compaction",id:"synchronous-compaction",children:[]}],u={toc:l};function m(e){var t=e.components,n=(0,i.Z)(e,r);return(0,a.kt)("wrapper",(0,o.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"Compaction is executed asynchronously with Hudi by default."),(0,a.kt)("h2",{id:"async-compaction"},"Async Compaction"),(0,a.kt)("p",null,"Async Compaction is performed in 2 steps:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("em",{parentName:"strong"},"Compaction Scheduling")),": This is done by the ingestion job. In this step, Hudi scans the partitions and selects ",(0,a.kt)("strong",{parentName:"li"},"file\nslices")," to be compacted. A compaction plan is finally written to Hudi timeline."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("em",{parentName:"strong"},"Compaction Execution")),": A separate process reads the compaction plan and performs compaction of file slices.")),(0,a.kt)("h2",{id:"deployment-models"},"Deployment Models"),(0,a.kt)("p",null,"There are few ways by which we can execute compactions asynchronously."),(0,a.kt)("h3",{id:"spark-structured-streaming"},"Spark Structured Streaming"),(0,a.kt)("p",null,"Compactions are scheduled and executed asynchronously inside the\nstreaming job.  Async Compactions are enabled by default for structured streaming jobs\non Merge-On-Read table."),(0,a.kt)("p",null,"Here is an example snippet in java"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-properties"},'import org.apache.hudi.DataSourceWriteOptions;\nimport org.apache.hudi.HoodieDataSourceHelpers;\nimport org.apache.hudi.config.HoodieCompactionConfig;\nimport org.apache.hudi.config.HoodieWriteConfig;\n\nimport org.apache.spark.sql.streaming.OutputMode;\nimport org.apache.spark.sql.streaming.ProcessingTime;\n\n\n DataStreamWriter<Row> writer = streamingInput.writeStream().format("org.apache.hudi")\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), "_row_key")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), "partition")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), "timestamp")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, "10")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), "true")\n        .option(HoodieWriteConfig.TABLE_NAME, tableName).option("checkpointLocation", checkpointLocation)\n        .outputMode(OutputMode.Append());\n writer.trigger(new ProcessingTime(30000)).start(tablePath);\n')),(0,a.kt)("h3",{id:"deltastreamer-continuous-mode"},"DeltaStreamer Continuous Mode"),(0,a.kt)("p",null,"Hudi DeltaStreamer provides continuous ingestion mode where a single long running spark application",(0,a.kt)("br",{parentName:"p"}),"\n","ingests data to Hudi table continuously from upstream sources. In this mode, Hudi supports managing asynchronous\ncompactions. Here is an example snippet for running in continuous mode with async compactions"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-properties"},"spark-submit --packages org.apache.hudi:hudi-utilities-bundle_2.11:0.6.0 \\\n--class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer \\\n--table-type MERGE_ON_READ \\\n--target-base-path <hudi_base_path> \\\n--target-table <hudi_table> \\\n--source-class org.apache.hudi.utilities.sources.JsonDFSSource \\\n--source-ordering-field ts \\\n--schemaprovider-class org.apache.hudi.utilities.schema.FilebasedSchemaProvider \\\n--props /path/to/source.properties \\\n--continous\n")),(0,a.kt)("h3",{id:"hudi-compactor-utility"},"Hudi Compactor Utility"),(0,a.kt)("p",null,"Hudi provides a standalone tool to execute specific compactions asynchronously. Below is an example and you can read more in the ",(0,a.kt)("a",{parentName:"p",href:"/docs/deployment#compactions"},"deployment guide")),(0,a.kt)("p",null,"Example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-properties"},"spark-submit --packages org.apache.hudi:hudi-utilities-bundle_2.11:0.6.0 \\\n--class org.apache.hudi.utilities.HoodieCompactor \\\n--base-path <base_path> \\\n--table-name <table_name> \\\n--schema-file <schema_file> \\\n--instant-time <compaction_instant>\n")),(0,a.kt)("p",null,"Note, the ",(0,a.kt)("inlineCode",{parentName:"p"},"instant-time")," parameter is now optional for the Hudi Compactor Utility. If using the utility without ",(0,a.kt)("inlineCode",{parentName:"p"},"--instant time"),",\nthe spark-submit will execute the earliest scheduled compaction on the Hudi timeline."),(0,a.kt)("h3",{id:"hudi-cli"},"Hudi CLI"),(0,a.kt)("p",null,"Hudi CLI is yet another way to execute specific compactions asynchronously. Here is an example and you can read more in the ",(0,a.kt)("a",{parentName:"p",href:"/docs/cli#compactions"},"deployment guide")),(0,a.kt)("p",null,"Example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-properties"},"hudi:trips->compaction run --tableName <table_name> --parallelism <parallelism> --compactionInstant <InstantTime>\n...\n")),(0,a.kt)("h2",{id:"synchronous-compaction"},"Synchronous Compaction"),(0,a.kt)("p",null,"By default, compaction is run asynchronously."),(0,a.kt)("p",null,"If latency of ingesting records is important for you, you are most likely using Merge-On-Read tables.\nMerge-On-Read tables store data using a combination of columnar (e.g parquet) + row based (e.g avro) file formats.\nUpdates are logged to delta files & later compacted to produce new versions of columnar files.\nTo improve ingestion latency, Async Compaction is the default configuration."),(0,a.kt)("p",null,"If immediate read performance of a new commit is important for you, or you want simplicity of not managing separate compaction jobs,\nyou may want Synchronous compaction, which means that as a commit is written it is also compacted by the same job."),(0,a.kt)("p",null,'Compaction is run synchronously by passing the flag "--disable-compaction" (Meaning to disable async compaction scheduling).\nWhen both ingestion and compaction is running in the same spark context, you can use resource allocation configuration\nin DeltaStreamer CLI such as ("--delta-sync-scheduling-weight",\n"--compact-scheduling-weight", ""--delta-sync-scheduling-minshare", and "--compact-scheduling-minshare")\nto control executor allocation between ingestion and compaction.'))}m.isMDXComponent=!0}}]);